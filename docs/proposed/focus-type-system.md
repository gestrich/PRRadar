# Focus Type System

## Background

The current focus area system only supports one type of focus: **method-level** grouping, where Claude identifies methods within diff hunks and creates one focus area per method. All rules are evaluated against these method-level focus areas regardless of whether that granularity is appropriate.

This is a weakness because different rules benefit from different slices of the diff:

- A **method implementation rule** needs to see the full method body to evaluate correctness, error handling, etc. (current behavior)
- An **architectural or convention rule** needs to see all changes in a file together to assess patterns, imports, organization, etc.

The rule should declare what kind of focus it needs, and the system should generate and pair the right focus areas accordingly.

For now, two focus types cover the primary use cases: `method` (one focus area per changed method) and `file` (one focus area per changed file, all changes grouped). Additional focus types (signature, documentation, etc.) can be added later as the system evolves.

Additionally, since PRRadar now operates with a local checkout (not just GitHub API), evaluations should leverage the actual codebase for additional context when the rule warrants it — not be limited to what's in the diff alone.

Finally, the `CodeSegment` class was removed during the focus areas implementation, but some "segment" terminology persists in method names, docstrings, and user-facing strings. This should be cleaned up for consistency.

**No backward compatibility:** This is a clean break. `RuleScope` is removed entirely and replaced by `focus_type`. No deprecation period, no migration shims. Existing rule files that use `scope` will need to switch to `focus_type`. Any code referencing `RuleScope` is deleted, not preserved.

---

## Proposed Focus Types

| Focus Type | Granularity | Description | Example Rule Use |
|---|---|---|---|
| `file` | One per changed file | All changes in a file grouped together | Architecture, conventions, import patterns, file organization |
| `method` | One per changed method | Full method body with context (current behavior) | Implementation correctness, error handling, method-specific logic |

Rules declare their focus type in YAML frontmatter. The system generates focus areas for each type that any loaded rule needs, then pairs rules with matching focus areas.

### Default Behavior

- If a rule does not declare `focus_type`, it defaults to `file`
- If no rules of a particular focus type are loaded, that type's generator is skipped entirely

---

## Phases

## - [x] Phase 1: FocusType Enum and Domain Model Updates

Add `FocusType` as a first-class concept to the domain layer.

**Tasks:**

1. Create `FocusType` enum in `domain/focus_area.py`:
   ```python
   class FocusType(Enum):
       FILE = "file"
       METHOD = "method"
   ```

2. Add `focus_type: FocusType` field to `FocusArea` dataclass (default: `FocusType.FILE`)

3. Update `FocusArea.to_dict()` and `FocusArea.from_dict()` for serialization round-trip

4. Tag all existing focus areas generated by the method detection logic as `FocusType.METHOD`

5. Export `FocusType` from `domain/__init__.py`

6. Update tests: verify serialization, default value, round-trip

**Files modified:**
- `prradar/domain/focus_area.py` — Added `FocusType` enum and `focus_type` field to `FocusArea`
- `prradar/domain/__init__.py` — Exported `FocusType`
- `prradar/services/focus_generator.py` — Tagged all generated focus areas (main + fallback) as `FocusType.METHOD`
- `tests/test_focus_type.py` — 16 new tests covering enum values, defaults, serialization round-trip, invalid/missing fallback, and generator tagging

**Technical notes:**
- `from_dict()` uses try/except on `FocusType(data["focus_type"])` to handle both missing keys (`KeyError`) and invalid values (`ValueError`), defaulting to `FocusType.FILE` — consistent with existing enum fallback patterns (e.g., `RuleScope`)
- All 501 tests pass (485 existing + 16 new)

---

## - [x] Phase 2: Rule Model — Add `focus_type` Field

Rules declare which focus type they need.

**Tasks:**

1. Add `focus_type: FocusType` field to `Rule` dataclass (default: `FocusType.FILE`)

2. Update `Rule.from_file()` to parse `focus_type` from YAML frontmatter:
   ```yaml
   ---
   description: Check error handling follows project conventions
   category: implementation
   focus_type: method
   ---
   ```

3. Update `Rule.from_dict()` and `Rule.to_dict()` for serialization

4. Invalid/missing `focus_type` values fall back to `FILE` (same pattern as `RuleScope`)

5. Remove `RuleScope` entirely — delete the `RuleScope` enum, remove the `scope` field from `Rule`, remove `scope` parsing from `from_file()` and `from_dict()`, remove `scope` serialization from `to_dict()`. Delete all `RuleScope` tests. Remove the export from `domain/__init__.py`.

6. Update tests: parsing with/without focus_type, round-trip, invalid values, remove all RuleScope tests

**Files modified:**
- `prradar/domain/rule.py` — Replaced `RuleScope` with `FocusType` import from `focus_area.py`; replaced `scope` field with `focus_type: FocusType` (default `FocusType.FILE`); updated `from_file()`, `from_dict()`, and `to_dict()` to use `focus_type`; removed `RuleScope` enum and `from enum import Enum`
- `prradar/domain/__init__.py` — Removed `RuleScope` import and `__all__` export
- `tests/test_diff_parser.py` — Replaced `TestRuleScope` (11 tests) with `TestRuleFocusType` (13 tests) covering: default value, explicit method/file, `to_dict` serialization, `from_dict` parsing (file, method, missing, invalid), `from_file` parsing (method, missing, invalid frontmatter), round-trip serialization for both types

**Technical notes:**
- `Rule` imports `FocusType` directly from `prradar.domain.focus_area` (not via `__init__.py`) to avoid circular imports
- Fallback pattern matches Phase 1's `FocusArea.from_dict()`: try/except on `FocusType(value)` for `ValueError`, `.get("focus_type", "file")` for missing keys
- Net test change: +2 (11 removed, 13 added) — 503 total tests pass (501 existing + 2 net new)

---

## - [x] Phase 3: File Focus Area Generator

Add file-level focus area generation alongside the existing method-level generator.

**Tasks:**

1. Add a `generate_file_focus_areas()` method to `FocusGeneratorService`. This groups all hunks for a given file into a single focus area with `FocusType.FILE`. No AI call needed — just aggregate hunks by file path, combine their content, and set `start_line`/`end_line` to cover the full range.

2. Update `FocusGeneratorService.generate_all_focus_areas()` to accept a set of requested `FocusType` values. For each requested type, run the appropriate generator:
   - `FocusType.METHOD` → existing Claude-based method detection (unchanged)
   - `FocusType.FILE` → new `generate_file_focus_areas()` (no AI, pure aggregation)

3. Return all generated focus areas tagged with their `focus_type`.

4. Update `FocusGenerationResult` to include focus areas of mixed types.

5. Tests: file focus area generation (single hunk file, multi-hunk file), mixed type generation, edge cases (empty hunks)

**Design notes:**
- File focus areas are free (no AI calls), so generating them adds no cost — `generation_cost_usd` will be 0 in the file output
- Method focus areas already track `generation_cost_usd` via `FocusGenerationResult`
- The `description` for file focus areas can be the file path itself
- `hunk_content` for file focus areas is the concatenation of all hunk contents for that file
- `focus_id` format: sanitized file path (e.g., `src-auth-handler.py`) with no hunk index since it covers the whole file

**Files modified:**
- `prradar/services/focus_generator.py` — Added `generate_file_focus_areas()` method (pure aggregation, no AI); updated `generate_all_focus_areas()` with optional `requested_types: set[FocusType]` parameter; added `defaultdict` import
- `tests/test_focus_type.py` — Added `TestFileFocusAreaGeneration` (7 tests: single-hunk, multi-hunk, multiple files, sanitized ID, hunk index tracking, empty hunks, annotated content) and `TestGenerateAllFocusAreasWithTypes` (5 tests: default METHOD-only, FILE-only skips method gen, both types mixed, file cost is zero, empty hunks)

**Technical notes:**
- `generate_all_focus_areas()` defaults to `requested_types={FocusType.METHOD}` when `None` — backward compatible with existing callers in `rules.py`
- `generate_file_focus_areas()` is synchronous (no AI calls needed), called directly from the async `generate_all_focus_areas()`
- Hunks grouped via `defaultdict(list)` keyed by `file_path`; annotated contents joined with `"\n\n"` separator
- `hunk_index` on file focus areas stores the index of the first hunk for that file (from the overall hunk list)
- `FocusGenerationResult` required no changes — it already supports mixed-type focus areas in its `focus_areas` list
- All 515 tests pass (503 existing + 12 new)

---

## - [x] Phase 4: Pipeline Updates — Type-Based Generation and Pairing

Update the rules command to generate focus areas by type and pair rules with matching focus areas.

**Tasks:**

1. In `cmd_rules()`, after loading all rules, determine the set of needed focus types:
   ```python
   needed_types = {rule.focus_type for rule in all_rules}
   ```

2. Generate focus areas for each needed type (call generators from Phase 3)

3. Save focus areas to separate files per type: `phase-2-focus-areas/method.json` and `phase-2-focus-areas/file.json`. Each file includes `generation_cost_usd` in its metadata.

4. When creating tasks, pair each rule with focus areas of matching type:
   ```python
   for rule in all_rules:
       matching_focus_areas = [fa for fa in all_focus_areas if fa.focus_type == rule.focus_type]
       for fa in matching_focus_areas:
           if rule_loader.filter_rules_for_focus_area([rule], fa):
               task = EvaluationTask.create(rule=rule, focus_area=fa)
   ```

5. Update `filter_rules_for_focus_area()` — the file pattern and grep filtering still applies on top of type matching

6. Update downstream consumers that read focus area files — load from the type-specific file matching the rule's `focus_type`. Remove references to `all.json`.

**Files modified:**
- `prradar/commands/agent/rules.py` — Restructured `cmd_rules()`: rules loaded before focus generation to determine `needed_types`; focus areas saved to per-type files (`method.json`, `file.json`); task pairing filters by `focus_type` before applying file/grep filters; `_fallback_focus_areas()` tags as `FocusType.METHOD`
- `prradar/services/phase_sequencer.py` — Replaced `FocusAreasPhaseChecker` from `_FixedFileChecker("all.json")` to dynamic checker that looks for any `*.json` type files; phase is complete when at least one type file exists
- `tests/test_phase_sequencer.py` — Updated `TestFocusAreasPhaseChecker` tests for type-specific files; added `test_complete_with_multiple_type_files` and `test_empty_directory`; updated dependency validation tests (`all.json` → `file.json`)
- `tests/test_focus_type.py` — Added `TestTypePairingInPipeline` (7 tests: file/method rules pair with matching type only, mixed rules, needed_types derivation, default rules, task creation preserves type), `TestFocusAreaPerTypeFiles` (3 tests: roundtrip, cost metadata, type isolation), `TestFallbackFocusAreasTaggedMethod` (1 test)

**Technical notes:**
- Rules are loaded before focus area generation (order change from prior implementation) so `needed_types` can be computed from the loaded rules
- `filter_rules_for_focus_area()` unchanged — type matching happens in the caller via `[r for r in all_rules if r.focus_type == focus_area.focus_type]` before calling `filter_rules_for_focus_area()` for file pattern/grep filtering
- No downstream consumers read `all.json` directly — focus areas are embedded within `EvaluationTask` objects, so removing `all.json` requires no downstream changes
- `FocusAreasPhaseChecker` now uses dynamic file detection (`*.json` glob) instead of fixed file list, matching `TasksPhaseChecker` pattern
- `generation_cost_usd` is attributed to METHOD type file only (FILE generation is free/no AI calls)
- All 528 tests pass (515 existing + 13 new, 2 existing removed, 2 existing updated)

---

## - [ ] Phase 5: Evaluation Enhancement — Codebase Exploration Context

Update the evaluation prompt to inform the AI about the local checkout and encourage codebase exploration when appropriate.

**Tasks:**

1. Update the evaluation prompt template in `evaluation_service.py` to include:
   - The local checkout path (the repo root where the PR branch is checked out)
   - Clear instructions that the codebase is available for exploration
   - Guidance on when to explore vs. when the focus area content is sufficient

2. Add the following context to the evaluation prompt (draft):
   ```
   ## Codebase Context

   The PR branch is checked out locally at: {repo_path}
   You have full access to the codebase for additional context.

   - For rules that evaluate isolated patterns (naming conventions, signature
     format), the focus area content above is typically sufficient.
   - For rules that evaluate broader concerns (architecture, client usage,
     integration patterns), explore the codebase as needed. For example,
     search for callers of a method, check how similar patterns are used
     elsewhere, or read surrounding code for context.

   Use your judgment: explore when it would improve the quality of your
   review, but don't explore unnecessarily for simple pattern checks.
   ```

3. Pass the repo path through the evaluation pipeline. It's already available from `repo.json` in the diff phase output — thread it through to `EvaluationService`.

4. Enable tool access in the `query()` call by adding `allowed_tools=["Read", "Grep", "Glob"]` and setting `cwd` to the repo checkout path. This lets Claude read files, search code, and explore the repo during evaluation before returning its structured verdict. The existing `output_format` for structured output still works alongside tool use — Claude explores as needed, then returns the JSON evaluation result.

5. Ensure `cost_usd` is captured from multi-turn evaluations. With tool access, a single evaluation may span multiple API turns. The `total_cost_usd` from the final message should reflect the full cost including tool-use turns. Verify this is already handled by the existing `ResultMessage.total_cost_usd` field.

6. Update the final summary report to include total cost across all phases:
   - Focus area generation cost (per type, from `generation_cost_usd` in each type's output file)
   - Evaluation cost per rule (from each evaluation result's `cost_usd`)
   - Total pipeline cost (sum of all above)

7. Update tests for the new prompt template and tool access configuration

**Files to modify:**
- `prradar/services/evaluation_service.py`
- `prradar/commands/agent/evaluate.py` (to pass repo path)
- Tests

---

## - [ ] Phase 6: Segment Terminology Cleanup

Remove remaining "segment" terminology from the codebase in favor of "focus area."

**Identified remnants from research:**

| Location | Current | Replace With |
|---|---|---|
| `domain/review.py` | `Feedback.segment: str` | `Feedback.focus_area_id: str` |
| `domain/review.py` | `ReviewSummary.total_segments: int` | `ReviewSummary.total_focus_areas: int` |
| `domain/rule.py` | `Rule.matches_diff_segment()` | `Rule.matches_diff_content()` |
| `services/rule_loader.py` | `filter_rules_for_segment()` | Delete entirely (replaced by `filter_rules_for_focus_area()`) |
| `services/github_comment.py` | `"Total Segments Reviewed:"` | `"Total Focus Areas Reviewed:"` |
| `commands/agent/analyze.py` | `violations_for_segment` variable | `violations_for_focus_area` |

**Tasks:**

1. Rename the methods and fields listed above
2. Update all callers and serialization
3. Update tests referencing the old names
4. Grep for any remaining "segment" references in `prradar/` and clean up

**Files to modify:**
- `prradar/domain/review.py`
- `prradar/domain/rule.py`
- `prradar/services/rule_loader.py`
- `prradar/services/github_comment.py`
- `prradar/commands/agent/analyze.py`
- Tests

---

## - [ ] Phase 7: Validation

**Automated testing:**
- Run full test suite: `python -m pytest tests/ -v`
- All existing tests pass (with updates from rename/refactor)
- New tests for each phase cover:
  - FocusType enum serialization and defaults
  - Rule focus_type parsing from YAML (file default, explicit method)
  - File focus area generator (single-hunk file, multi-hunk file)
  - Type-based pairing in the pipeline (file rules get file focus areas, method rules get method focus areas)
  - Updated evaluation prompt content
  - Renamed methods/fields

**Integration testing:**
- Run `prradar agent analyze` on a real PR with rules of both focus types
- Verify that a rule with `focus_type: method` is paired with method-level focus areas
- Verify that a rule with no `focus_type` (defaults to `file`) is paired with file-level focus areas
- Check that the evaluation prompt includes codebase context

**Edge cases to verify:**
- Rule with no `focus_type` defaults to `file`
- Rule with invalid `focus_type` defaults to `file`
- No rules of a given type — that generator is skipped
- Separate output files per focus type round-trip correctly (`method.json`, `file.json`)
- All rules default to `file` when no `focus_type` specified — method generator not invoked unless a rule explicitly requests it

---

## Open Questions

1. ~~**Should Claude have tool access during evaluation?**~~ **Yes.** Cost is not a consideration. Phase 5 should enable tool access by adding `allowed_tools=["Read", "Grep", "Glob"]` and setting `cwd` to the repo checkout path in the `query()` options. Claude can then read files, search code, and explore the repo during evaluation before returning its structured verdict.

2. **Future focus types:** Additional types like `signature`, `file_signatures`, `documentation`, and `hunk` could be added later as the system evolves. The enum and pipeline are designed to be extensible.
